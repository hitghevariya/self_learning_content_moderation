# self_learning_content_moderation
This project implements an industry-grade toxic / abusive content detection pipeline that goes beyond static machine learning models. The system automatically detects novel (out-of-distribution) language, discovers new open-source datasets, and re-trains itself without human intervention.
